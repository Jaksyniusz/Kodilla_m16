{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e703b25-dc0e-438d-ae48-9bc0ed0e0a20",
   "metadata": {},
   "source": [
    "# Unsupervised Learning\n",
    "## 1. Wprowadzenie\n",
    "Unsupervised Learning jest sposobem nauki, w którym nie mamy \"nauczyciela\" mówiącego nam, jakie odpowiedzi są prawidłowe, a jakie nie.\n",
    "Prosiliśmy o zaznaczenie definicji nieopisujących Unsupervised Learning, więc należało wybrać a i b.\n",
    "\n",
    "Unsupervised Learning można podzielić na dwie główne podgrupy:\n",
    "- Clustering – czyli klateryzacja, która polega na podziale naszego zbioru na podobne elementy. Przykładem takiego zadania może być podzielenie kredek na poszczególne kolory np. wszystkie odcienie czerwonego są w jednym miejscu, wszystkie odcienie niebieskiego są w innym miejscu\n",
    "\n",
    "- Dimensionality Reduction – czyli redukcja wymiarowości jest techniką często stosowaną w przypadku danych zawierających wiele cech (features), z czego wiele z nich może być zbędnych.\n",
    "\n",
    "## 2. Rozszerzona definicja Unsupervised Learning\n",
    "### Podział Unsupervised Learning\n",
    "#### Clustering\n",
    "Służy do znajdowania elementów naszego zbioru posiadających wspólne cechy. Można to porównać do segregacji prania, gdzie dzielimy ubrania ze względu na kolor, stopień zabrudzenia itp. na poszczególne podgrupy.\n",
    "#### Dimensionality Reduction\n",
    "Służy, jak sama nazwa wskazuje, do zmniejszania wymiarowości danych. Załóżmy, że mamy zbiór danych, na którym mamy wykonać analizę statystyczną, aby określić kto najczęściej odwiedza nasze forum. Jednak ktoś nie pofatygował się, zrobił kopiuj-wklej z bazy danych do Excela i tutaj pojawia się zgrzyt.\n",
    "\n",
    "Dostaliśmy zbiór, który posiada kilkadziesiąt cech np. ilość napisanych słów, najczęściej używane słowo, najczęściej polubione posty, ile razy zmieniano hasło, kiedy ostatnio ktoś logował się na dane konto.\n",
    "\n",
    "W naszym kontekście duża ilość tych kolumn jest zbędna. Owszem, można wyrzucać je ręcznie, ale posiadając kolumny o nazwie np. \"częstotliwość logowania się użytkownika na podforum\" oraz \"częstotliwość logowania się na subforum\", oraz \"częstotliwość logowania się na forum\" oraz \"częstotliwość logowania się\", nie wiemy, która z nich jest istotniejsza od innych i usuwanie którejś ręcznie może okazać się strzałem w dziesiątkę albo pudłem.\n",
    "\n",
    "## 3. Crossvalidation – po co, dlaczego działa?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addcf716-f287-4510-9b97-a2727f2b6a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "X, y, iris_classes = iris.data, iris.target, iris.target_names\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=10)\n",
    "\n",
    "for fold_nr, (train_idx, test_idx) in enumerate(kf.split(X)):\n",
    "  X_train = X[train_idx]\n",
    "  X_test = X[test_idx]\n",
    "\n",
    "  y_train = y[train_idx]\n",
    "  y_test = y[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9b8bf5-0a14-4fb9-8c50-f24771c2da59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "models = []\n",
    "scores = []\n",
    "\n",
    "for fold_nr, (train_idx, test_idx) in enumerate(kf.split(X)):\n",
    "  X_train = X[train_idx]\n",
    "  X_test =X[test_idx]\n",
    "\n",
    "  y_train = y[train_idx]\n",
    "  y_test = y[test_idx]\n",
    "\n",
    "  # skalowanie danych wejściowych, aby model lepiej działał\n",
    "  X_train = scaler.fit_transform(X_train)\n",
    "  X_test = scaler.transform(X_test)\n",
    "\n",
    "  clf = SGDClassifier(random_state=1).fit(X_train, y_train)\n",
    "\n",
    "  models.append(clf)\n",
    "  scores.append(clf.score(X_test, y_test))\n",
    "\n",
    "\n",
    "print(\"wyniki poszczególnych foldów: \", scores)\n",
    "print(\"średni wynik wszystkich foldów: \", np.array(scores).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d7b845-1e16-42d7-9f85-598e01f0aaf2",
   "metadata": {},
   "source": [
    "wyniki poszczególnych foldów:  [0.8666666666666667, 0.9333333333333333, 0.9666666666666667, 0.9, 0.9333333333333333]\n",
    "średni wynik wszystkich foldów:  0.9199999999999999\n",
    "Pojedynczy wynik:  0.8222222222222222\n",
    "\n",
    "### Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29093c49-724d-4f27-9e43-720e9cd3cfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "\n",
    "# stworzenie klasyfikatora\n",
    "clf = SGDClassifier(random_state=1)\n",
    "\n",
    "# użycie metody cross_val_score do sprawdzenia\n",
    "# działania naszego modelu na różnych podziałach\n",
    "cv_score = cross_val_score(clf, X, y, cv=5)\n",
    "print(\"wynik kroswalidacji: \", cv_score)\n",
    "print(\"średni wynik wszystkich foldów: \", cv_score.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8dccdf1-2c2f-42f0-a95a-cced02377f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = \\\n",
    "  train_test_split(X, y, test_size=0.2, stratify=X[\"ilość_kupionych_butów\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7009689-0117-4f20-b85b-3864ec2ad3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_classes=2,\n",
    "    weights=[0.99, 0.01],\n",
    "    flip_y=0,\n",
    "    random_state=1\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size = 0.2, random_state=3)\n",
    "\n",
    "train_0, train_1 = len(y_train[y_train==0]), len(y_train[y_train==1])\n",
    "test_0, test_1 = len(y_test[y_test==0]), len(y_test[y_test==1])\n",
    "\n",
    "print('>Train: 0=%d, 1=%d, Test: 0=%d, 1=%d' % (train_0, train_1, test_0, test_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a652587a-42fd-424a-9b99-a39f36a14ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    ">Train: 0=790, 1=10, Test: 0=200, 1=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4843c559-ddaa-49ad-9ece-53ab13ad7a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size = 0.2, random_state=3, stratify=y)\n",
    "\n",
    "train_0, train_1 = len(y_train[y_train==0]), len(y_train[y_train==1])\n",
    "test_0, test_1 = len(y_test[y_test==0]), len(y_test[y_test==1])\n",
    "\n",
    "print('>Train: 0=%d, 1=%d, Test: 0=%d, 1=%d' % (train_0, train_1, test_0, test_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c1de0a-07c0-4605-8646-48be4dfae81b",
   "metadata": {},
   "outputs": [],
   "source": [
    ">Train: 0=792, 1=8, Test: 0=198, 1=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a79c12d-2ee1-4e29-adb5-f1b2b23b71ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# ... dotąd tak samo, jak w poprzednim przykładzie\n",
    "\n",
    "# dodatkowy parametr y w metodzie split\n",
    "for fold_nr, (train_idx, test_idx) in enumerate(kf.split(X, y)):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8302135-b48e-4dde-82ca-c784f4284d96",
   "metadata": {},
   "source": [
    "## 4. Clustering\n",
    "### Definicja\n",
    "Definicja ta jest bardzo podobna do definicji klasyfikacji, w której również grupujemy elementy, lecz tam zawczasu znamy klasy, do których dany element powinien należeć. W przypadku Clusteringu nie mamy na wejściu takiej informacji.\n",
    "### Do czego się stosuje\n",
    "Clustering ma wiele zastosowań. Jest używany między innymi do:\n",
    " -segmentacji grup kupujących, aby widzieć, jakie są grupy docelowe naszej nowej kampanii promocyjnej oraz na czym skupić uwagę podczas jej projektowania,\n",
    "- rekomendacji np. podobnych playlist na Spotify czy YouTube,\n",
    "- wykrywania anomalii: wybieramy te grupy, które posiadają wielu członków, natomiast maluteńkie (względem pozostałych grup) traktujemy jako anomalie i odrzucamy,\n",
    "- wyszukiwania podobnych elementów wśród np. zdjęć (podobny rozkład pikseli).\n",
    "### Podstawowe metody (KMeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309b3ae2-8680-4876-9e18-619090db2ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# środki naszych klastrów\n",
    "\n",
    "centroids = np.array([\n",
    "    [ 0.8, 2.0],\n",
    "    [-0.5, 2.0],\n",
    "    [-2.0, 2.0],\n",
    "    [-2.5, 2.5],\n",
    "    [-2.5, 1.0]\n",
    "])\n",
    "\n",
    "# wprowadzenie szumu do naszych klastrów, aby rozrzucić próbki\n",
    "blob_std = np.array([0.4, 0.3, 0.1, 0.1, 0.1])\n",
    "\n",
    "# stworzenie zbioru danych\n",
    "X, y = make_blobs(\n",
    "    n_samples=3000,\n",
    "    centers=centroids,\n",
    "    cluster_std=blob_std,\n",
    "    random_state=7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e29879-bddb-40ef-a575-42ee19b329b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "clf = KMeans(n_clusters=5)\n",
    "\n",
    "# możemy użyć metod fit(), predict()\n",
    "clf.fit(X)\n",
    "y_pred = clf.predict(X)\n",
    "\n",
    "# albo metody fit)predict, która łączy dwie powyższe\n",
    "y_pred = clf.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f51722c-18da-4e3b-8b0d-5e1ddfabe773",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plot_decision_boundaries(clf, X)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37986d26-5ff5-4488-8fbe-ab95cf3fde08",
   "metadata": {},
   "source": [
    "W praktyce K-means źle radzi sobie ze zbiorami gdzie \"bloby\", grupy danych, mają różne średnice. Bierze się to z jego sposobu działania, który przypisuje dane do klastrów stosując zasadę, że do jakiej \"centroidy\", czyli środka danej grupy jest punktowi najbliżej, tam zostanie on zaklasyfikowany."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b0d47f-20d6-466e-8579-9d3202807e9f",
   "metadata": {},
   "source": [
    "### Zasada działania KMeans\n",
    "- algorytm wybiera losowo centroidy, czyli środki poszczególnych grup,\n",
    "- przypisuje punkty do grup, biorąc pod uwagę najmniejszą odległość od centroidy,\n",
    "- poprawia pozycję centroid tak, aby najbardziej pasowały do danych,\n",
    "- powtarza od punktu 2 aż do momentu, kiedy centroidy przestaną się ruszać,\n",
    "- algorytm wykorzystuje wybór losowych centroidów, zatem algorytm jest uruchamiany kilkukrotnie, żeby zweryfikować najczęstszą zbieżność.\n",
    "\n",
    "### Szybsze wersje KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbaecea-48e2-4324-8218-21a99b96d49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = np.array([\n",
    "    [ 0.8, 2.0],\n",
    "    [-0.5, 2.0],\n",
    "    [-2.0, 2.0],\n",
    "    [-2.5, 2.5],\n",
    "    [-2.5, 1.0]\n",
    "])\n",
    "\n",
    "blob_std = np.array([0.4, 0.3, 0.1, 0.1, 0.1])\n",
    "\n",
    "# 3 miliony punktów!!!\n",
    "X, y = make_blobs(\n",
    "    n_samples=30000000,\n",
    "    centers=centroids,\n",
    "    cluster_std=blob_std,\n",
    "    random_state=7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed21ad9-5509-4152-9549-0322ae837043",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "%timeit\n",
    "\n",
    "# metoda MiniBatchKMeans\n",
    "MBKMeans_clf = MiniBatchKMeans(n_clusters=5, max_iter=10, random_state=1)\n",
    "MBKMeans_clf.fit(X)\n",
    "\n",
    "1 loop, best of 3: **11.8 s per loop**\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "%timeit\n",
    "\n",
    "# metoda KMeans\n",
    "KMeans_clf = KMeans(n_clusters=5, max_iter=10, random_state=1)\n",
    "KMeans_clf.fit(X)\n",
    "\n",
    "1 loop, best of 3: **15.9 s per loop**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c232a2d-d557-4684-8034-1784497dd861",
   "metadata": {},
   "source": [
    "### Jak dobrać ilość klastrów?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fc2471-691f-4668-bb69-548dd1fad883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tworzymy 19 algorytmów z k od 1 do 19\n",
    "kmeans_per_k = [\n",
    "    KMeans(n_clusters=k, random_state=2).fit(X)\n",
    "    for k in range(2, 20) # zaczynamy od wartości n_clusters wynoszącej 2, gdyż nie ma sensu dzielenia zbioru dla 1 klastra\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e1e00d-c4d2-4892-8e53-9d10230919bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "inertias = [model.inertia_ for model in kmeans_per_k]\n",
    "plt.plot(range(2, 20), inertias, 'bx-')\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('SSE')\n",
    "plt.title('Elbow Method using SSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a0b67d-3c21-4afc-a65d-7173d1bfcf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "silhouette_scores = [\n",
    "    silhouette_score(X, model.labels_)\n",
    "    for model in kmeans_per_k\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c52b28-1fd1-42d8-a831-18f1ddcf82ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(range(2, 10), silhouette_scores, \"bo-\")\n",
    "plt.ylabel(\"Silhouette score\", fontsize=14)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb29a8f-ed3a-43cc-8433-883e75dab7e9",
   "metadata": {},
   "source": [
    "KMeans we wszystkich odmianach posiada jedną zasadniczą wadę: źle działa dla danych, które nie są zbite w sferyczne grupy. Jeżeli mamy dane, które układają się np. elipsoidalnie (rozrzut wartości w grupie jest różny w różnych kierunkach), KMeans nie będzie działał już tak dobrze.\n",
    "\n",
    "### Praktyczne użycie KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0413a8c0-255a-45eb-83bb-f075f4fd887a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "X_digits, y_digits = load_digits(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X_digits, y_digits, random_state=2)\n",
    "\n",
    "log_reg = LogisticRegression(multi_class=\"ovr\", max_iter=5000, random_state=2)\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "print(\"Bez KMeans :\", log_reg.score(X_test, y_test))\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"kmeans\", KMeans(\n",
    "        n_clusters=40,\n",
    "        random_state=2\n",
    "    )),\n",
    "    (\"log_reg\", LogisticRegression(\n",
    "        multi_class=\"ovr\",\n",
    "        max_iter=5000,\n",
    "        random_state=2\n",
    "    )),\n",
    "])\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "print(\"Z KMeans :\", pipeline.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e71235c-46b8-46ee-a8b6-379c9520ce49",
   "metadata": {},
   "source": [
    "Bez KMeans : 0.9466666666666667\n",
    "Z KMeans : 0.9622222222222222"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5beaefb-0351-463b-b22f-2b2d4c04e1f7",
   "metadata": {},
   "source": [
    "Jest prostym algorytmem do podziału danych.\n",
    "\n",
    "Nadaje się do przeskalowanych danych, które są ułożone w zwartą formę.\n",
    "\n",
    "Słabo działa w przypadku danych nieprzeskalowanych lub niesferycznych.\n",
    "\n",
    "Powinien być pierwszym wyborem do przetestowania czy i jak zbiór się dzieli. Zazwyczaj, dla pewności, bierze się kilka razy więcej klastrów niż mamy klas.\n",
    "\n",
    "W praktyce n_init trzeba ustawić na wartość inną niż domyślna 1, aby model dobrze działał. Jest to związane z tym, że model ten ma tendencję do znajdowania czasami złych rozwiązań. Hiperparmetr ten mówi nam, ile razy sklearn ma odpalić algorytm i zachować tylko najlepszy wynik. Im więcej razy go puścimy, tym wynik będzie lepszy, jednak algorytm będzie dłużej działał.\n",
    "\n",
    "### DBSCAN\n",
    "\n",
    "Algorytm ten dobrze działa, jeśli dane są gęsto rozmieszczone i jest wyraźna separacja między poszczególnymi klastrami.\n",
    "\n",
    "Ponadto pozwala on odsiać wszystkie anomalie, czyli dane odbiegające wartościami od pozostałych.\n",
    "\n",
    "Jest to przydatne dlatego, że podczas nauki modelu takie anomalie potrafią wiele namieszać i można spędzić długie godziny, poprawiając model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15c6ac7-8922-4a77-8deb-d95130a4b93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "# tworzymy zbiór moon\n",
    "X, y = make_moons(n_samples=1000, noise=0.08)\n",
    "\n",
    "# tworzymy pierwszy klasyfikator DBSCAN z eps = 0.05\n",
    "dbscan = DBSCAN(eps=0.05, min_samples=5)\n",
    "dbscan.fit(X)\n",
    "\n",
    "# drugi DBSCAN z eps = 0.2 (większy epsilon, większa przestrzeń wokół)\n",
    "dbscan_2 = DBSCAN(eps=0.2, min_samples=5)\n",
    "dbscan_2.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79a7f76-4a21-4e3e-8132-4482fa28a200",
   "metadata": {},
   "outputs": [],
   "source": [
    "KMeans_clf = KMeans(n_clusters=2, max_iter=100000)\n",
    "KMeans_clf.fit(X)\n",
    "\n",
    "plot_decision_boundaries(KMeans_clf, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2742e23d-d77f-4fd5-9570-0bbf7b9d3391",
   "metadata": {},
   "source": [
    "Jest to prosty algorytm, który posiada tylko dwa hiperparametry:\n",
    "- epsilon\n",
    "- min_samples\n",
    "\n",
    "Z odpowiednio dobranymi parametrami potrafi dopasować się do KAŻDEGO zbioru.\n",
    "\n",
    "Potrafi wykrywać anomalie (outliers).\n",
    "\n",
    "Nie posiada jednak metody predict() – jeżeli chcemy zobaczyć, gdzie wpasowują się nowe dane, musimy przeszkolić klasyfikator na nowo ze zbiorem zawierającym stare i nowe dane.\n",
    "\n",
    "Wada:\n",
    "- Wymaga O(m^2) pamięci (m — ilość elementów), ale jest bardzo szybki: O(m*log(m)).\n",
    "\n",
    "### Gaussian Mixture Models (GNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb59b3d2-8052-4fe1-b152-680e5123c089",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "import numpy as np\n",
    "\n",
    "# tworzymy \"bloby\", czyli grupy danych\n",
    "X1, y1 = make_blobs(n_samples=800, centers=((2, -2), (-2, 2)), random_state=2)\n",
    "X1 = X1.dot(np.array([[0.374, 0.95], [0.732, 0.598]]))\n",
    "X2, y2 = make_blobs(n_samples=200, centers=1, random_state=42)\n",
    "X2 = X2 + [6, -8]\n",
    "\n",
    "# przydatna metoda NumPy r_ - służy do składania dwóch tablic wg wybranej osi\n",
    "X = np.r_[X1, X2]\n",
    "y = np.r_[y1, y2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5730b99-b2d0-40ce-803d-411d8837b46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gaussian_mixture(clf, X, resolution=1000):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f22157-fc15-4e4a-b8f6-6b92d8803793",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "gm = GaussianMixture(n_components=3, n_init=10, random_state=42)\n",
    "gm.fit(X)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "\n",
    "plot_gaussian_mixture(gm, X)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611a1e5a-8ebe-434b-9ed3-3d6621a37887",
   "metadata": {},
   "outputs": [],
   "source": [
    "gm.coverged_\n",
    "gm.n_iter_\n",
    "gm.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f12d68c-241d-46c0-87da-94b5a821441b",
   "metadata": {},
   "source": [
    "### GMM jako wykrywacz anomalii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5c00b7-d0bd-4635-9f25-e9bab46c6c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pobieramy wartości dla naszego zbioru, które\n",
    "# mówią nam, jak bardzo element należy do klastrów\n",
    "density = gm.score_samples(X)\n",
    "\n",
    "# Tworzymy wartość graniczną, czyli threshold. Wartości,\n",
    "# które mają wyniki z poprzedniej linii mniejsze od naszego\n",
    "# threshold, są anomaliami, więc zostają odsiane\n",
    "\n",
    "# W naszym przypadku chcemy odsiać 5% najgorszych wartości\n",
    "threshold = np.percentile(density, 5)\n",
    "\n",
    "# Odsiewamy te wartości poprzez proste porównanie\n",
    "anomalie = X[density < threshold]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baafb79b-3a43-4813-ba46-14eaf10f2c58",
   "metadata": {},
   "source": [
    "Usuwanie anomalii stosuje się często w następujący sposób:\n",
    "- skalowanie,\n",
    "- sprawdzanie, czy są anomalie,\n",
    "- trenowanie modelu z anomaliami,\n",
    "- usuwanie anomalii,\n",
    "- trenowanie nowego modelu bez anomalii,\n",
    "- sprawdzanie, który model jest lepszy,\n",
    "- wybór lepszego modelu.\n",
    "\n",
    "## 5. Redukcja wymiarowości\n",
    "### Analiza głównych składowych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf51cb0-e00b-4c91-bebc-e69279853420",
   "metadata": {},
   "outputs": [],
   "source": [
    "means = [2.13, -99, 0.11, -1.01] # cztery zmienne z innymi średnimi\n",
    "\n",
    "# Macierz kowariancji\n",
    "cov_matrix = np.array([[1, 0.96, 0, 0], [0.96, 1, 0, 0], [0, 0, 1, 0.87], [0, 0, 0.87, 1]])\n",
    "print('Covariance matrix')\n",
    "print(cov_matrix)\n",
    "\n",
    "n=1000 # Liczba wierszy\n",
    "df = np.random.multivariate_normal(means, cov_matrix, n)\n",
    "print('Dataframe:')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56361557-e7ac-479a-a641-3b3e4bf2c69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 16))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "img = ax.scatter(xs=df[:,0], ys=df[:,1], zs=df[:,2], c=df[:,3], s=60)\n",
    "cax = fig.add_axes([ax.get_position().x1+0.10, ax.get_position().y0+0.14, 0.02, ax.get_position().height*0.3])\n",
    "fig.colorbar(img, cax=cax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d55da3c-9085-4069-8197-e941e97c84c5",
   "metadata": {},
   "source": [
    "#### Standaryzacja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2892ed-7a6a-447f-91ec-ea545498abb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Przed standaryzacją')\n",
    "print('Średnia:\\n',df.mean(axis=0))\n",
    "print('Odchylenie standardowe:\\n',df.std(axis=0))\n",
    "\n",
    "# Standaryzacja\n",
    "df_standardizated = (df - np.mean(df, axis=0)) / np.std(df, axis=0)\n",
    "\n",
    "print('\\nPo standaryzacji')\n",
    "print('Średnia:\\n',df_standardizated.mean(axis=0))\n",
    "print('Odchylenie standardowe:\\n',df_standardizated.std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cb2323-2548-4d58-a8fa-04fafc65d7a2",
   "metadata": {},
   "source": [
    "#### Utworzenie macierzy kowariancji w oparciu o wystandaryzowaną macierz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30792fd-2d94-4a1c-883a-a4b2a9db559a",
   "metadata": {},
   "outputs": [],
   "source": [
    "covariance_matrix = np.cov(df_standardizated.T)\n",
    "covariance_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ac6cd0-9599-43f4-8a14-7e65333bbd70",
   "metadata": {},
   "source": [
    "#### Wektory i Wartości własne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45707c08-5223-4b63-b24b-1255dea7fdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvalues, eigenvectors = np.linalg.eig(covariance_matrix)\n",
    "print('Wartości własne:\\n',eigenvalues,'\\n\\nWektory własne:\\n',eigenvectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75e219a-872a-4fd5-a1ca-7b5792afddf3",
   "metadata": {},
   "source": [
    "#### Wyjaśniona wariancja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f3f7c1-f655-4e78-8670-cc92c6b36776",
   "metadata": {},
   "outputs": [],
   "source": [
    "explained_variance = [round((i/np.sum(eigenvalues)), 3) for i in sorted(eigenvalues, reverse=True)]\n",
    "print(explained_variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c08c85-46fa-42fe-853b-2e7b79e13c91",
   "metadata": {},
   "source": [
    "#### Transformacja cech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ff7c51-0b7d-47fd-9098-568ff3a37657",
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenpairs = [(np.abs(eigenvalues[i]), eigenvectors[:, i]) for i in range(len(eigenvalues))]\n",
    "eigenpairs.sort(key = lambda k: k[0], reverse=True)\n",
    "w = np.hstack((eigenpairs[0][1][:, np.newaxis],\n",
    "               eigenpairs[1][1][:, np.newaxis]))\n",
    "pc1 = df.dot(w.T[0])\n",
    "pc2 = df.dot(w.T[1])\n",
    "fig = plt.figure(figsize=(16, 8))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(x=pc1, y=pc2, c='black', s=60)\n",
    "ax.set_xlabel(xlabel='PC1', rotation=0, loc='center', size=15)\n",
    "ax.set_ylabel(ylabel='PC2', rotation=90, loc='center', size=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027dd7f1-d303-4cc8-bc15-418173539ff4",
   "metadata": {},
   "source": [
    "### PCA - zastosowanie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c56581-7d80-4f34-b1d6-cc32ca468456",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "iris = sns.load_dataset('iris')\n",
    "iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ef4141-042c-4d72-9875-d63b2dce0b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris['species'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2f208b-b2a8-4bb4-9460-de3708b87b4b",
   "metadata": {},
   "source": [
    "### Wizualizacja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc262abe-b7be-4a3a-b191-cf567441dfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(13, 13))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "img = ax.scatter(xs=iris.loc[iris['species']=='virginica', 'sepal_length'],\n",
    "                 ys=iris.loc[iris['species']=='virginica','sepal_width'],\n",
    "                 zs=iris.loc[iris['species']=='virginica', 'petal_length'],\n",
    "                 s=iris.loc[iris['species']=='virginica','petal_width']*50,\n",
    "                 c='red', label='virginica')\n",
    "img = ax.scatter(xs=iris.loc[iris['species']=='setosa', 'sepal_length'],\n",
    "                 ys=iris.loc[iris['species']=='setosa','sepal_width'],\n",
    "                 zs=iris.loc[iris['species']=='setosa', 'petal_length'],\n",
    "                 s=iris.loc[iris['species']=='setosa','petal_width']*50,\n",
    "                 c='green', label='setosa')\n",
    "img = ax.scatter(xs=iris.loc[iris['species']=='versicolor', 'sepal_length'],\n",
    "                 ys=iris.loc[iris['species']=='versicolor','sepal_width'],\n",
    "                 zs=iris.loc[iris['species']=='versicolor', 'petal_length'],\n",
    "                 s=iris.loc[iris['species']=='versicolor','petal_width']*50,\n",
    "                 c='blue', label='versicolor')\n",
    "ax.set_xlabel(xlabel='sepal length', size=15)\n",
    "ax.set_ylabel(ylabel='sepal width', size=15)\n",
    "ax.set_zlabel(zlabel='petal length', size=15)\n",
    "ax.set_title('Rozmiar punktu: petal width', size=15)\n",
    "plt.legend(title='Species')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248f5c8c-d2f5-4029-87cc-07b3d4df9025",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(iris,\n",
    "             hue='species',\n",
    "             palette={'virginica': 'red', 'setosa': 'green', 'versicolor': 'blue'})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8bc7bf-05cd-4111-91cb-fa03f8758582",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 12))\n",
    "ax = sns.heatmap(iris.corr(),\n",
    "                 xticklabels=iris.corr().columns,\n",
    "                 yticklabels=iris.corr().columns,\n",
    "                 cmap='RdYlGn',\n",
    "                 center=0,\n",
    "                 annot=True,\n",
    "                 vmin=-1,\n",
    "                 vmax= 1)\n",
    "\n",
    "plt.title('Korelacja dla zmiennych z IRIS', fontsize=24)\n",
    "bottom, top = ax.get_ylim()\n",
    "ax.set_ylim(bottom + 0.5, top - 0.5)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662522af-3f1d-4511-8e07-4b993c7b27b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "X = iris.drop('species', axis=1).copy()\n",
    "y = iris['species'].copy()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42, stratify=y)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaler = scaler.fit_transform(X_train)\n",
    "pca = PCA(random_state=42)\n",
    "X_train_pca = pca.fit_transform(X_train_scaler)\n",
    "train_iris = pd.DataFrame(np.concatenate([X_train_pca,\n",
    "                          np.array(y_train).reshape(-1, 1)],\n",
    "                          axis=1))\n",
    "train_iris.rename(columns = {0: 'PC1', 1: 'PC2',\n",
    "                             2: 'PC3', 3: 'PC4', 4: 'species'},\n",
    "                  inplace=True)\n",
    "train_iris[['PC1', 'PC2','PC3','PC4']] = train_iris[['PC1', 'PC2','PC3', 'PC4']].astype(float)\n",
    "train_iris.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785beeca-113f-4224-b95c-a65e9ba85bc2",
   "metadata": {},
   "source": [
    "#### Grupowanie cech – jakie zmienne przedstawiają tę samą informację"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41c5222-2af6-41d6-8201-ea437a7e5a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "plt.imshow(pca.components_.T, cmap = 'Spectral', vmin =-1, vmax = 1)\n",
    "plt.yticks(range(len(X_train.columns)), X_train.columns, fontsize=12)\n",
    "plt.xticks(range(4), range(1, 5), fontsize=12)\n",
    "plt.xlabel('Główne Składowe', fontsize=15)\n",
    "plt.ylabel('Zmienne', fontsize=15)\n",
    "plt.title('Rozkład zmiennych według głównych składowych ~ PCA', fontsize=20)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a12a925-a1ff-48e0-bb46-e446707d4f7e",
   "metadata": {},
   "source": [
    "#### Redukcja wymiaru bez znacznej utraty informacji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a691236-1194-477b-a484-53046e4d7d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,8))\n",
    "fig.subplots_adjust(wspace=.4, hspace=.4)\n",
    "ax = fig.add_subplot(2, 1, 1)\n",
    "ax.bar(range(1, 1+pca.n_components_), pca.explained_variance_ratio_, color='black')\n",
    "ax.set(xticks=[1, 2, 3, 4])\n",
    "plt.yticks(np.arange(0, 1.1, 0.1))\n",
    "plt.title('Wyjaśniona wariancja', fontsize=15)\n",
    "plt.xlabel('Główne Składowe', fontsize=13)\n",
    "plt.ylabel('% wyjaśnionej wariancji', fontsize=13)\n",
    "ax = fig.add_subplot(2, 1, 2)\n",
    "ax.bar(range(1, 1+pca.n_components_), np.cumsum(pca.explained_variance_ratio_), color='black')\n",
    "ax.set(xticks=[1, 2, 3, 4])\n",
    "plt.yticks(np.arange(0, 1.1, 0.1))\n",
    "plt.title('Skumulowanana Wyjaśniona wariancja', fontsize=15)\n",
    "plt.xlabel('Główne Składowe', fontsize=13)\n",
    "plt.ylabel('% wyjaśnionej wariancji', fontsize=13)\n",
    "plt.show()\n",
    "\n",
    "principal_component = 1\n",
    "cum_explained_var = 0\n",
    "for explained_var in pca.explained_variance_ratio_:\n",
    "    cum_explained_var += explained_var\n",
    "    print(f'Główna składowa: {principal_component}, Wyjaśniona wariancja: {np.round(explained_var, 5)}, Skumulowana Wyjaśniona wariancja: {np.round(cum_explained_var, 5)}')\n",
    "    principal_component += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a3ab8b-eaaa-46de-b6c6-bc92f361f481",
   "metadata": {},
   "source": [
    "#### Wizualizacja wielowymiarowych danych z wykorzystaniem redukcji zmiennych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9356d6-6bab-4d6b-b2a7-d19a10bb5693",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 12))\n",
    "plt.scatter(x=train_iris.loc[train_iris['species']=='virginica', 'PC1'],\n",
    "            y=train_iris.loc[train_iris['species']=='virginica','PC2'],\n",
    "            c='red', label='virginica', s=50)\n",
    "plt.scatter(x=train_iris.loc[train_iris['species']=='setosa', 'PC1'],\n",
    "            y=train_iris.loc[train_iris['species']=='setosa','PC2'],\n",
    "            c='green', label='setosa', s=50)\n",
    "plt.scatter(x=train_iris.loc[train_iris['species']=='versicolor', 'PC1'],\n",
    "            y=train_iris.loc[train_iris['species']=='versicolor','PC2'],\n",
    "            c='blue', label='versicolor', s=50)\n",
    "plt.xlabel(xlabel='PC1', size=15)\n",
    "plt.ylabel(ylabel='PC2', size=15)\n",
    "plt.title('Wykres Głównych Składowych', size=20)\n",
    "plt.legend(title='Species')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d876b59-6971-4df0-8009-a22f5163feab",
   "metadata": {},
   "source": [
    "Redukcja wymiarowości jako inżyniera cech przed Uczeniem Nadzorowanym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305f676a-c26a-40f7-9eb2-e3959a8a3e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from prettytable import PrettyTable\n",
    "import datetime\n",
    "\n",
    "X_test_scaler = scaler.transform(X_test)\n",
    "X_test_pca = pca.transform(X_test_scaler)\n",
    "\n",
    "def train_and_check(Xtrain, Xtest, ytrain, ytest):\n",
    "    classifier = LogisticRegression(max_iter=100000)\n",
    "    start = datetime.datetime.now()\n",
    "    classifier.fit(Xtrain, ytrain)\n",
    "    end = datetime.datetime.now()\n",
    "    time = (end - start).microseconds\n",
    "    evaluation = np.round(classifier.score(Xtest, ytest), 4)\n",
    "    return evaluation, time\n",
    "\n",
    "results = PrettyTable(['Model',\n",
    "                       'Dokładność',\n",
    "                       'Czas trenowania (microseconds)'])\n",
    "\n",
    "# Trenowanie modelu na nieprzetworzonym zbiorze\n",
    "not_scaled_data = train_and_check(X_train, X_test, y_train, y_test)\n",
    "results.add_row(['Nieskalowane dane', not_scaled_data[0], not_scaled_data[1]])\n",
    "\n",
    "# Trenowanie modelu na przetworzonym zbiorze\n",
    "scaled_data = train_and_check(X_train_scaler, X_test_scaler, y_train, y_test)\n",
    "results.add_row(['Skalowane dane', scaled_data[0], scaled_data[1]])\n",
    "\n",
    "# Trenowanie modelu na czterech Głównych Składowych\n",
    "PC4_data = train_and_check(X_train_pca, X_test_pca, y_train, y_test)\n",
    "results.add_row(['4 PC', PC4_data[0], PC4_data[1]])\n",
    "\n",
    "# Trenowanie modelu na trzech Głównych Składowych\n",
    "PC3_data = train_and_check(X_train_pca[:, :3], X_test_pca[:, :3], y_train, y_test)\n",
    "results.add_row(['3 PC', PC3_data[0], PC3_data[1]])\n",
    "\n",
    "# Trenowanie modelu na dwóch Głównych Składowych\n",
    "PC2_data = train_and_check(X_train_pca[:, :2], X_test_pca[:, :2], y_train, y_test)\n",
    "results.add_row(['2 PC', PC2_data[0], PC2_data[1]])\n",
    "\n",
    "# Trenowanie modelu na jednej Głównej Składowej\n",
    "PC1_data = train_and_check(X_train_pca[:, :1], X_test_pca[:, :1],  y_train, y_test)\n",
    "results.add_row(['1 PC', PC1_data[0], PC1_data[1]])\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
